{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jb3EGTiSj_Er",
        "outputId": "792981a6-5bfb-41d4-8f41-327a6158e26b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/100.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/353.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m353.7/353.7 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/337.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.3/337.3 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/992.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m992.7/992.7 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.8/324.8 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q \"dlt[qdrant]\" \"qdrant-client[fastembed]\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1. dlt Version\n",
        "\n",
        "In this homework, we will load the data from our FAQ to Qdrant\n",
        "\n",
        "Let's install dlt with Qdrant support and Qdrant client:\n",
        "\n",
        "What's the version of dlt that you installed?"
      ],
      "metadata": {
        "id": "S3pypIP-lCSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show dlt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYEgFAzykEz7",
        "outputId": "a91b655d-35bc-48c8-b85a-3abde60a236c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: dlt\n",
            "Version: 1.14.1\n",
            "Summary: dlt is an open-source python-first scalable data loading library that does not require any backend to run.\n",
            "Home-page: https://github.com/dlt-hub\n",
            "Author: \n",
            "Author-email: \"dltHub Inc.\" <services@dlthub.com>\n",
            "License: \n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: click, fsspec, gitpython, giturlparse, hexbytes, humanize, jsonpath-ng, orjson, packaging, pathvalidate, pendulum, pluggy, pytz, pyyaml, requests, requirements-parser, rich-argparse, semver, setuptools, simplejson, sqlglot, tenacity, tomlkit, typing-extensions, tzdata\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# dlt Resourse\n",
        "For reading the FAQ data, we have this helper function:\n",
        "\n",
        "Annotate it with @dlt.resource. We will use it when creating a dlt pipeline."
      ],
      "metadata": {
        "id": "kS3DA2cClGfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dlt\n",
        "import os\n",
        "import requests\n"
      ],
      "metadata": {
        "id": "18hUQlhClgsx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dlt.resource(table_name=\"api_source_docs\", max_table_nesting=0)\n",
        "def zoomcamp_data():\n",
        "    docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
        "    docs_response = requests.get(docs_url)\n",
        "    documents_raw = docs_response.json()\n",
        "\n",
        "    for course in documents_raw:\n",
        "        course_name = course['course']\n",
        "\n",
        "        for doc in course['documents']:\n",
        "            doc['course'] = course_name\n",
        "            yield doc"
      ],
      "metadata": {
        "id": "49AKHUaklJ_h"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2. dlt pipeline\n",
        "Now let's create a pipeline.\n",
        "\n",
        "We need to define a destination for that. Let's use the qdrant one:"
      ],
      "metadata": {
        "id": "GqM-QXGmlUDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dlt.destinations import qdrant\n",
        "\n",
        "qdrant_destination = qdrant(\n",
        "  qd_path=\"db.qdrant\",\n",
        ")"
      ],
      "metadata": {
        "id": "qMUciJd9lTl-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we tell dlt (and Qdrant) to create a folder with our data, and the name for it will be db.qdrant\n",
        "\n",
        "Let's run it:"
      ],
      "metadata": {
        "id": "XmtAPZcAlYFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"zoomcamp_pipeline\",\n",
        "    destination=qdrant_destination,\n",
        "    dataset_name=\"zoomcamp_tagged_data\"\n",
        "\n",
        ")\n",
        "load_info = pipeline.run(zoomcamp_data())\n",
        "print(pipeline.last_trace)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgvFbpz6lWzZ",
        "outputId": "c6d6e56a-30b7-48b7-844b-0d00f684bfb5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run started at 2025-08-05 09:36:14.811297+00:00 and COMPLETED in 11.86 seconds with 4 steps.\n",
            "Step extract COMPLETED in 0.36 seconds.\n",
            "\n",
            "Load package 1754386575.3422406 is EXTRACTED and NOT YET LOADED to the destination and contains no failed jobs\n",
            "\n",
            "Step normalize COMPLETED in 0.13 seconds.\n",
            "Normalized data for the following tables:\n",
            "- _dlt_pipeline_state: 1 row(s)\n",
            "- api_source_docs: 948 row(s)\n",
            "\n",
            "Load package 1754386575.3422406 is NORMALIZED and NOT YET LOADED to the destination and contains no failed jobs\n",
            "\n",
            "Step load COMPLETED in 10.86 seconds.\n",
            "Pipeline zoomcamp_pipeline load step completed in 10.83 seconds\n",
            "1 load package(s) were loaded to destination qdrant and into dataset zoomcamp_tagged_data\n",
            "The qdrant destination used /content/db.qdrant location to store data\n",
            "Load package 1754386575.3422406 is LOADED and contains no failed jobs\n",
            "\n",
            "Step run COMPLETED in 11.86 seconds.\n",
            "Pipeline zoomcamp_pipeline load step completed in 10.83 seconds\n",
            "1 load package(s) were loaded to destination qdrant and into dataset zoomcamp_tagged_data\n",
            "The qdrant destination used /content/db.qdrant location to store data\n",
            "Load package 1754386575.3422406 is LOADED and contains no failed jobs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many rows were inserted into the zoomcamp_data collection?\n",
        "\n",
        "Look for \"Normalized data for the following tables:\" in the trace output."
      ],
      "metadata": {
        "id": "RWLblQXDlbag"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a2890eb",
        "outputId": "4b2cdfaa-7e2e-4fbc-9677-8cc6fb2c9998"
      },
      "source": [
        "import re\n",
        "\n",
        "trace_output = \"\"\"\n",
        "Run started at 2025-08-05 09:36:14.811297+00:00 and COMPLETED in 11.86 seconds with 4 steps.\n",
        "Step extract COMPLETED in 0.36 seconds.\n",
        "\n",
        "Load package 1754386575.3422406 is EXTRACTED and NOT YET LOADED to the destination and contains no failed jobs\n",
        "\n",
        "Step normalize COMPLETED in 0.13 seconds.\n",
        "Normalized data for the following tables:\n",
        "- _dlt_pipeline_state: 1 row(s)\n",
        "- api_source_docs: 948 row(s)\n",
        "\n",
        "Load package 1754386575.3422406 is NORMALIZED and NOT YET LOADED to the destination and contains no failed jobs\n",
        "\n",
        "Step load COMPLETED in 10.86 seconds.\n",
        "Pipeline zoomcamp_pipeline load step completed in 10.83 seconds\n",
        "1 load package(s) were loaded to destination qdrant and into dataset zoomcamp_tagged_data\n",
        "The qdrant destination used /content/db.qdrant location to store data\n",
        "Load package 1754386575.3422406 is LOADED and contains no failed jobs\n",
        "\n",
        "Step run COMPLETED in 11.86 seconds.\n",
        "Pipeline zoomcamp_pipeline load step completed in 10.83 seconds\n",
        "1 load package(s) were loaded to destination qdrant and into dataset zoomcamp_tagged_data\n",
        "The qdrant destination used /content/db.qdrant location to store data\n",
        "Load package 1754386575.3422406 is LOADED and contains no failed jobs\n",
        "\"\"\"\n",
        "\n",
        "match = re.search(r\"- api_source_docs: (\\d+) row\\(s\\)\", trace_output)\n",
        "if match:\n",
        "    rows_inserted = match.group(1)\n",
        "    print(f\"Number of rows inserted into api_source_docs: {rows_inserted}\")\n",
        "else:\n",
        "    print(\"Could not find the number of rows inserted.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows inserted into api_source_docs: 948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3. Embeddings\n",
        "\n",
        "When inserting the data, an embedding model was used. Which one?\n",
        "\n",
        "You can find this out by inspecting the meta.json file created in the target folder. During the data insertion process, a folder named db.qdrant will be created, and the meta.json file will be located inside this folder."
      ],
      "metadata": {
        "id": "WbHi13dgmvQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qdrant_destination"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvI3vwgGwfCt",
        "outputId": "d71e32cf-d06a-47c5-9213-d13afd8b7357"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<dlt.destinations.qdrant(destination_type='qdrant', staging_dataset_name_layout='%s_staging', enable_dataset_name_normalization=True, info_tables_query_threshold=1000, local_dir='/content', pipeline_name='zoomcamp_pipeline', pipeline_working_dir='/var/dlt/pipelines/zoomcamp_pipeline', qd_path='db.qdrant', dataset_separator='_', embedding_batch_size=32, embedding_parallelism=0, upload_batch_size=64, upload_parallelism=1, upload_max_retries=3, model='BAAI/bge-small-en')>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "metadata_string = \"<dlt.destinations.qdrant(destination_type='qdrant', staging_dataset_name_layout='%s_staging', enable_dataset_name_normalization=True, info_tables_query_threshold=1000, local_dir='/content', pipeline_name='zoomcamp_pipeline', pipeline_working_dir='/var/dlt/pipelines/zoomcamp_pipeline', qd_path='db.qdrant', dataset_separator='_', embedding_batch_size=32, embedding_parallelism=0, upload_batch_size=64, upload_parallelism=1, upload_max_retries=3, model='BAAI/bge-small-en')>\"\n",
        "\n",
        "match = re.search(r\"model='(.*?)'\", metadata_string)\n",
        "if match:\n",
        "    model_name = match.group(1)\n",
        "    print(f\"The embedding model used is: {model_name}\")\n",
        "else:\n",
        "    print(\"Could not find the embedding model name in the metadata string.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-piJ3hXuxVBQ",
        "outputId": "891071bb-3216-4ab2-995d-2300213abed2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The embedding model used is: BAAI/bge-small-en\n"
          ]
        }
      ]
    }
  ]
}